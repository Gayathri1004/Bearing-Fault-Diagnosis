{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CWRU Bearing Fault Diagnosis with Spectrogram and Scalogram Inputs\n",
    "\n",
    "This notebook implements a Convolutional Neural Network (CNN) for fault diagnosis of rolling element bearings using the Case Western Reserve University (CWRU) dataset. The methodology is based on Verstraete et al. (2017), which uses time-frequency image representations (spectrograms via STFT and scalograms via Morlet wavelet transform) as inputs to a CNN. The CNN architecture features double convolutional layers before pooling to enhance feature expressivity.\n",
    "\n",
    "\n",
    "## Overview\n",
    "- **Dataset**: CWRU Bearing Data (2HP load, various fault types and sizes)\n",
    "- **Input**: 96x96 pixel spectrogram or scalogram images\n",
    "- **Model**: CNN with three stages of double conv layers (32, 64, 128 filters), max pooling, and two dense layers\n",
    "- **Training**: 5-fold stratified cross-validation with early stopping\n",
    "- **Evaluation**: Accuracy, confusion matrices, and fold-wise performance plots\n",
    "\n",
    "## Requirements\n",
    "- Python libraries: tensorflow, numpy, scipy, scikit-learn, matplotlib, seaborn, pywt, skimage\n",
    "- CWRU dataset files in 'CWRU_BearingData_Load_2HP' folder\n",
    "\n",
    "Run cells sequentially to load data, preprocess, train the model, and evaluate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for Reproducibility\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set a fixed seed value for reproducibility\n",
    "SEED = 1\n",
    "random.seed(SEED)            # Python random module\n",
    "np.random.seed(SEED)         # NumPy\n",
    "tf.random.set_seed(SEED)     # TensorFlow\n",
    "\n",
    "# Enforce deterministic behavior for GPU operations\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'  # Ensure deterministic execution\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'  # Deterministic cuDNN algorithms\n",
    "\n",
    "# Control GPU memory allocation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)  # Enable memory growth\n",
    "\n",
    "# Restrict parallelism\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Import additional libraries\n",
    "import scipy.io\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from skimage.transform import resize\n",
    "import pywt  # For wavelet transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CWRU Bearing Data\n",
    "\n",
    "The `ImportData` function loads the CWRU dataset files (.mat) containing vibration signals for normal and fault conditions (inner race, ball, outer race) at various fault sizes under a 2HP load. The data is stored in a list for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImportData():\n",
    "    folder_path = 'CWRU_BearingData_Load_2HP'\n",
    "    file_paths = [\n",
    "        os.path.join(folder_path, '99.mat'),\n",
    "        os.path.join(folder_path, '111.mat'),\n",
    "        os.path.join(folder_path, '124.mat'),\n",
    "        os.path.join(folder_path, '137.mat'),\n",
    "        os.path.join(folder_path, '176.mat'),\n",
    "        os.path.join(folder_path, '191.mat'),\n",
    "        os.path.join(folder_path, '203.mat'),\n",
    "        os.path.join(folder_path, '215.mat'),\n",
    "        os.path.join(folder_path, '228.mat'),\n",
    "        os.path.join(folder_path, '240.mat')\n",
    "    ]\n",
    "    data_keys = [\n",
    "        'X099_DE_time', 'X111_DE_time', 'X124_DE_time', 'X137_DE_time',\n",
    "        'X176_DE_time', 'X191_DE_time', 'X203_DE_time', 'X215_DE_time',\n",
    "        'X228_DE_time', 'X240_DE_time'\n",
    "    ]\n",
    "    data = [scipy.io.loadmat(fp)[key] for fp, key in zip(file_paths, data_keys)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Frequency Image Generation\n",
    "\n",
    "The following functions generate spectrogram and scalogram images from vibration signal segments, as per Verstraete et al. (2017):\n",
    "- **Spectrogram**: Uses Short-Time Fourier Transform (STFT) with a Hann window, 256-point segments, and 128-point overlap. Images are normalized and resized to 96x96 pixels.\n",
    "- **Scalogram**: Uses Continuous Wavelet Transform (CWT) with the Morlet wavelet, scales 1 to 128. Magnitude is normalized and resized to 96x96 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_norm(ary):\n",
    "    ary = (ary - ary.min()) / np.abs(ary.max() - ary.min())\n",
    "    return ary\n",
    "\n",
    "def generate_spectrogram_image(data_y_vector, image_shape=(96, 96)):\n",
    "    \"\"\"\n",
    "    Calculate the spectrogram of an array data_y_vector and resize it to image_shape.\n",
    "    \"\"\"\n",
    "    fs = 48000\n",
    "    f, t, sxx = signal.spectrogram(\n",
    "        data_y_vector,\n",
    "        fs,\n",
    "        nperseg=256,\n",
    "        noverlap=128,\n",
    "        window='hann'\n",
    "    )\n",
    "    sxx = min_max_norm(sxx)\n",
    "    sxx = resize(sxx, image_shape, mode='constant', anti_aliasing=True)\n",
    "    return sxx\n",
    "\n",
    "def generate_scalogram_image(data_y_vector, image_shape=(96, 96)):\n",
    "    \"\"\"\n",
    "    Calculate the scalogram using Morlet wavelet and resize it to image_shape.\n",
    "    \"\"\"\n",
    "    fs = 48000\n",
    "    scales = np.arange(1, 128)  # Adjust scales for Morlet wavelet\n",
    "    coef, freqs = pywt.cwt(\n",
    "        data_y_vector,\n",
    "        scales,\n",
    "        'morl',\n",
    "        sampling_period=1/fs\n",
    "    )\n",
    "    coef = np.abs(coef)  # Use magnitude\n",
    "    coef = min_max_norm(coef)\n",
    "    coef = resize(coef, image_shape, mode='constant', anti_aliasing=True)\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The `Sampling` function segments the vibration signals into blocks, and `DataPreparation` generates spectrogram or scalogram images for each segment. Labels are created in one-hot encoded format (`Y_CNN`) for CNN training and as integers (`Y`) for stratification. The output is a 4D array of images and corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sampling(Data, interval_length, samples_per_block, ignore_points=0):\n",
    "    adjusted_length = len(Data) - 2 * ignore_points\n",
    "    No_of_blocks = (round(adjusted_length / interval_length) - round(samples_per_block / interval_length) - 1)\n",
    "    SplitData = np.zeros([No_of_blocks, samples_per_block])\n",
    "    for i in range(No_of_blocks):\n",
    "        start_idx = ignore_points + i * interval_length\n",
    "        SplitData[i, :] = Data[start_idx:(start_idx + samples_per_block)].T\n",
    "    return SplitData\n",
    "\n",
    "def DataPreparation(Data, interval_length, samples_per_block, image_type='spectrogram'):\n",
    "    \"\"\"\n",
    "    Prepare data by generating spectrogram or scalogram images.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    LabelPositional = []\n",
    "    Label = []\n",
    "    for count, data in enumerate(Data):\n",
    "        SplitData = Sampling(data, interval_length, samples_per_block)\n",
    "        images = []\n",
    "        for segment in SplitData:\n",
    "            if image_type == 'spectrogram':\n",
    "                img = generate_spectrogram_image(segment)\n",
    "            elif image_type == 'scalogram':\n",
    "                img = generate_scalogram_image(segment)\n",
    "            images.append(img)\n",
    "        images = np.array(images)\n",
    "        y = np.zeros([len(SplitData), 10])\n",
    "        y[:, count] = 1\n",
    "        y1 = np.zeros([len(SplitData), 1])\n",
    "        y1[:, 0] = count\n",
    "        X.append(images)\n",
    "        LabelPositional.append(y)\n",
    "        Label.append(y1)\n",
    "    X = np.concatenate(X, axis=0)\n",
    "    LabelPositional = np.concatenate(LabelPositional, axis=0)\n",
    "    Label = np.concatenate(Label, axis=0)\n",
    "    return X, LabelPositional, Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Definition\n",
    "\n",
    "The `CNN_2D` class defines the CNN architecture as per Verstraete et al. (2017):\n",
    "- Three stages, each with two convolutional layers (32, 64, 128 filters, 3x3 kernels) followed by 2x2 max pooling.\n",
    "- Two dense layers (100 units each) with dropout (0.5) before the second dense layer.\n",
    "- Output layer with 10 units (softmax) for 10-class classification.\n",
    "- Compiled with Adam optimizer and categorical crossentropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_2D():\n",
    "    def __init__(self):\n",
    "        self.model = self.CreateModel()\n",
    "\n",
    "    def CreateModel(self):\n",
    "        model = models.Sequential([\n",
    "            layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(96, 96, 1)),\n",
    "            layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "            layers.MaxPool2D((2, 2), padding='same'),\n",
    "            layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "            layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "            layers.MaxPool2D((2, 2), padding='same'),\n",
    "            layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "            layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "            layers.MaxPool2D((2, 2), padding='same'),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(100, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(96, activation='relu'),\n",
    "            layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "\n",
    "This section executes the pipeline:\n",
    "- Loads and preprocesses data to generate 96x96 images (spectrogram or scalogram).\n",
    "- Splits data into training (80%) and test (20%) sets with stratification.\n",
    "- Performs 5-fold cross-validation with early stopping and model checkpointing.\n",
    "- Saves the best model for each fold.\n",
    "\n",
    "**Parameters**:\n",
    "- `image_type`: Set to 'scalogram' or 'spectrogram' to choose input type.\n",
    "- `image_shape`: 96x96 pixels for high accuracy (per Verstraete et al.).\n",
    "- `kSplits`: 5 for cross-validation.\n",
    "- `foldername`: Directory to save model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input Data: (15169, 96, 96, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Gayathri/pyenvs/tf-env/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 11:07:24.606482: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n",
      "2025-06-11 11:07:24.626218: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5915 - loss: 1.1129"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 11:13:40.347425: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n",
      "2025-06-11 11:13:40.348333: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.98929, saving model to CNN2D_results/Scalogram_Spectrogram/best_model_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 1s/step - accuracy: 0.5921 - loss: 1.1113 - val_accuracy: 0.9893 - val_loss: 0.0361\n",
      "Epoch 2/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9284 - loss: 0.1969\n",
      "Epoch 2: val_accuracy did not improve from 0.98929\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m641s\u001b[0m 2s/step - accuracy: 0.9285 - loss: 0.1969 - val_accuracy: 0.9559 - val_loss: 0.1197\n",
      "Epoch 3/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9507 - loss: 0.1517\n",
      "Epoch 3: val_accuracy did not improve from 0.98929\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m772s\u001b[0m 3s/step - accuracy: 0.9507 - loss: 0.1516 - val_accuracy: 0.9839 - val_loss: 0.0515\n",
      "Epoch 4/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9522 - loss: 0.1319\n",
      "Epoch 4: val_accuracy did not improve from 0.98929\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 1s/step - accuracy: 0.9522 - loss: 0.1319 - val_accuracy: 0.9794 - val_loss: 0.0677\n",
      "Epoch 5/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9617 - loss: 0.1091\n",
      "Epoch 5: val_accuracy did not improve from 0.98929\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 1s/step - accuracy: 0.9617 - loss: 0.1091 - val_accuracy: 0.9839 - val_loss: 0.0572\n",
      "Epoch 6/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9640 - loss: 0.0994\n",
      "Epoch 6: val_accuracy did not improve from 0.98929\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 1s/step - accuracy: 0.9641 - loss: 0.0993 - val_accuracy: 0.9798 - val_loss: 0.0721\n",
      "Epoch 7/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9638 - loss: 0.0898\n",
      "Epoch 7: val_accuracy did not improve from 0.98929\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 1s/step - accuracy: 0.9638 - loss: 0.0897 - val_accuracy: 0.9864 - val_loss: 0.0454\n",
      "Epoch 8/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9759 - loss: 0.0680\n",
      "Epoch 8: val_accuracy improved from 0.98929 to 0.99094, saving model to CNN2D_results/Scalogram_Spectrogram/best_model_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 1s/step - accuracy: 0.9759 - loss: 0.0680 - val_accuracy: 0.9909 - val_loss: 0.0221\n",
      "Epoch 9/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9740 - loss: 0.0710\n",
      "Epoch 9: val_accuracy did not improve from 0.99094\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 1s/step - accuracy: 0.9740 - loss: 0.0710 - val_accuracy: 0.9881 - val_loss: 0.0365\n",
      "Epoch 10/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9772 - loss: 0.0666\n",
      "Epoch 10: val_accuracy did not improve from 0.99094\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 1s/step - accuracy: 0.9772 - loss: 0.0666 - val_accuracy: 0.9740 - val_loss: 0.0785\n",
      "Epoch 11/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9763 - loss: 0.0664\n",
      "Epoch 11: val_accuracy did not improve from 0.99094\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 1s/step - accuracy: 0.9763 - loss: 0.0664 - val_accuracy: 0.9909 - val_loss: 0.0284\n",
      "Epoch 12/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9743 - loss: 0.0602\n",
      "Epoch 12: val_accuracy improved from 0.99094 to 0.99217, saving model to CNN2D_results/Scalogram_Spectrogram/best_model_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 1s/step - accuracy: 0.9743 - loss: 0.0602 - val_accuracy: 0.9922 - val_loss: 0.0236\n",
      "Epoch 13/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9809 - loss: 0.0504\n",
      "Epoch 13: val_accuracy improved from 0.99217 to 0.99629, saving model to CNN2D_results/Scalogram_Spectrogram/best_model_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 1s/step - accuracy: 0.9809 - loss: 0.0504 - val_accuracy: 0.9963 - val_loss: 0.0087\n",
      "Epoch 14/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9851 - loss: 0.0425\n",
      "Epoch 14: val_accuracy did not improve from 0.99629\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 1s/step - accuracy: 0.9851 - loss: 0.0425 - val_accuracy: 0.9963 - val_loss: 0.0116\n",
      "Epoch 15/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9814 - loss: 0.0534\n",
      "Epoch 15: val_accuracy improved from 0.99629 to 0.99670, saving model to CNN2D_results/Scalogram_Spectrogram/best_model_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 1s/step - accuracy: 0.9814 - loss: 0.0534 - val_accuracy: 0.9967 - val_loss: 0.0108\n",
      "Epoch 16/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9842 - loss: 0.0409\n",
      "Epoch 16: val_accuracy did not improve from 0.99670\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 1s/step - accuracy: 0.9842 - loss: 0.0409 - val_accuracy: 0.9942 - val_loss: 0.0165\n",
      "Epoch 17/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9838 - loss: 0.0470\n",
      "Epoch 17: val_accuracy did not improve from 0.99670\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 1s/step - accuracy: 0.9838 - loss: 0.0470 - val_accuracy: 0.9951 - val_loss: 0.0124\n",
      "Epoch 18/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9856 - loss: 0.0422\n",
      "Epoch 18: val_accuracy improved from 0.99670 to 0.99794, saving model to CNN2D_results/Scalogram_Spectrogram/best_model_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 1s/step - accuracy: 0.9856 - loss: 0.0422 - val_accuracy: 0.9979 - val_loss: 0.0073\n",
      "Epoch 19/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9887 - loss: 0.0320\n",
      "Epoch 19: val_accuracy did not improve from 0.99794\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2718s\u001b[0m 9s/step - accuracy: 0.9887 - loss: 0.0320 - val_accuracy: 0.9975 - val_loss: 0.0083\n",
      "Epoch 20/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9885 - loss: 0.0331\n",
      "Epoch 20: val_accuracy did not improve from 0.99794\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 1s/step - accuracy: 0.9885 - loss: 0.0331 - val_accuracy: 0.9975 - val_loss: 0.0086\n",
      "Epoch 21/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9888 - loss: 0.0388\n",
      "Epoch 21: val_accuracy did not improve from 0.99794\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 1s/step - accuracy: 0.9888 - loss: 0.0388 - val_accuracy: 0.9951 - val_loss: 0.0133\n",
      "Epoch 22/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9872 - loss: 0.0352\n",
      "Epoch 22: val_accuracy improved from 0.99794 to 0.99835, saving model to CNN2D_results/Scalogram_Spectrogram/best_model_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1024s\u001b[0m 3s/step - accuracy: 0.9872 - loss: 0.0352 - val_accuracy: 0.9984 - val_loss: 0.0074\n",
      "Epoch 23/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9847 - loss: 0.0519\n",
      "Epoch 23: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 1s/step - accuracy: 0.9847 - loss: 0.0518 - val_accuracy: 0.9975 - val_loss: 0.0073\n",
      "Epoch 24/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9923 - loss: 0.0235\n",
      "Epoch 24: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 1s/step - accuracy: 0.9923 - loss: 0.0236 - val_accuracy: 0.9963 - val_loss: 0.0113\n",
      "Epoch 25/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9884 - loss: 0.0304\n",
      "Epoch 25: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 1s/step - accuracy: 0.9884 - loss: 0.0304 - val_accuracy: 0.9967 - val_loss: 0.0116\n",
      "Epoch 26/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9902 - loss: 0.0284\n",
      "Epoch 26: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 1s/step - accuracy: 0.9902 - loss: 0.0284 - val_accuracy: 0.9934 - val_loss: 0.0226\n",
      "Epoch 27/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9908 - loss: 0.0262\n",
      "Epoch 27: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 1s/step - accuracy: 0.9908 - loss: 0.0262 - val_accuracy: 0.9979 - val_loss: 0.0090\n",
      "Epoch 28/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9887 - loss: 0.0354\n",
      "Epoch 28: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 1s/step - accuracy: 0.9887 - loss: 0.0355 - val_accuracy: 0.9975 - val_loss: 0.0101\n",
      "Epoch 29/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9901 - loss: 0.0280\n",
      "Epoch 29: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 1s/step - accuracy: 0.9901 - loss: 0.0280 - val_accuracy: 0.9963 - val_loss: 0.0110\n",
      "Epoch 30/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9903 - loss: 0.0291\n",
      "Epoch 30: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 1s/step - accuracy: 0.9904 - loss: 0.0291 - val_accuracy: 0.9971 - val_loss: 0.0108\n",
      "Epoch 31/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9960 - loss: 0.0124\n",
      "Epoch 31: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 1s/step - accuracy: 0.9960 - loss: 0.0125 - val_accuracy: 0.9951 - val_loss: 0.0138\n",
      "Epoch 32/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9892 - loss: 0.0331\n",
      "Epoch 32: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 1s/step - accuracy: 0.9892 - loss: 0.0331 - val_accuracy: 0.9971 - val_loss: 0.0142\n",
      "Epoch 33/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9919 - loss: 0.0227\n",
      "Epoch 33: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 1s/step - accuracy: 0.9919 - loss: 0.0227 - val_accuracy: 0.9975 - val_loss: 0.0092\n",
      "Epoch 34/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9919 - loss: 0.0250\n",
      "Epoch 34: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 1s/step - accuracy: 0.9919 - loss: 0.0250 - val_accuracy: 0.9984 - val_loss: 0.0072\n",
      "Epoch 35/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9923 - loss: 0.0226\n",
      "Epoch 35: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 1s/step - accuracy: 0.9924 - loss: 0.0225 - val_accuracy: 0.9959 - val_loss: 0.0211\n",
      "Epoch 36/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9957 - loss: 0.0147\n",
      "Epoch 36: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 1s/step - accuracy: 0.9957 - loss: 0.0147 - val_accuracy: 0.9984 - val_loss: 0.0116\n",
      "Epoch 37/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9927 - loss: 0.0224\n",
      "Epoch 37: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 1s/step - accuracy: 0.9927 - loss: 0.0225 - val_accuracy: 0.9922 - val_loss: 0.0265\n",
      "Epoch 38/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9904 - loss: 0.0285\n",
      "Epoch 38: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 1s/step - accuracy: 0.9904 - loss: 0.0285 - val_accuracy: 0.9967 - val_loss: 0.0127\n",
      "Epoch 39/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9947 - loss: 0.0167\n",
      "Epoch 39: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 1s/step - accuracy: 0.9947 - loss: 0.0167 - val_accuracy: 0.9963 - val_loss: 0.0126\n",
      "Epoch 40/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9933 - loss: 0.0197\n",
      "Epoch 40: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 1s/step - accuracy: 0.9933 - loss: 0.0197 - val_accuracy: 0.9967 - val_loss: 0.0085\n",
      "Epoch 41/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9940 - loss: 0.0190\n",
      "Epoch 41: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 1s/step - accuracy: 0.9940 - loss: 0.0190 - val_accuracy: 0.9975 - val_loss: 0.0122\n",
      "Epoch 42/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9950 - loss: 0.0130\n",
      "Epoch 42: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 1s/step - accuracy: 0.9950 - loss: 0.0130 - val_accuracy: 0.9984 - val_loss: 0.0077\n",
      "Epoch 43/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9945 - loss: 0.0172\n",
      "Epoch 43: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m515s\u001b[0m 2s/step - accuracy: 0.9945 - loss: 0.0173 - val_accuracy: 0.9951 - val_loss: 0.0150\n",
      "Epoch 44/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9872 - loss: 0.0382\n",
      "Epoch 44: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 1s/step - accuracy: 0.9872 - loss: 0.0382 - val_accuracy: 0.9963 - val_loss: 0.0088\n",
      "Epoch 45/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9920 - loss: 0.0212\n",
      "Epoch 45: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 1s/step - accuracy: 0.9920 - loss: 0.0212 - val_accuracy: 0.9979 - val_loss: 0.0096\n",
      "Epoch 46/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9932 - loss: 0.0208\n",
      "Epoch 46: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 1s/step - accuracy: 0.9932 - loss: 0.0208 - val_accuracy: 0.9967 - val_loss: 0.0114\n",
      "Epoch 47/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9952 - loss: 0.0135\n",
      "Epoch 47: val_accuracy improved from 0.99835 to 0.99876, saving model to CNN2D_results/Scalogram_Spectrogram/best_model_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 1s/step - accuracy: 0.9952 - loss: 0.0135 - val_accuracy: 0.9988 - val_loss: 0.0041\n",
      "Epoch 48/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9972 - loss: 0.0108\n",
      "Epoch 48: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 1s/step - accuracy: 0.9972 - loss: 0.0108 - val_accuracy: 0.9959 - val_loss: 0.0119\n",
      "Epoch 49/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9942 - loss: 0.0145\n",
      "Epoch 49: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 1s/step - accuracy: 0.9942 - loss: 0.0145 - val_accuracy: 0.9984 - val_loss: 0.0051\n",
      "Epoch 50/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9938 - loss: 0.0190\n",
      "Epoch 50: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 1s/step - accuracy: 0.9938 - loss: 0.0191 - val_accuracy: 0.9967 - val_loss: 0.0130\n",
      "Epoch 51/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9931 - loss: 0.0214\n",
      "Epoch 51: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 1s/step - accuracy: 0.9931 - loss: 0.0214 - val_accuracy: 0.9971 - val_loss: 0.0108\n",
      "Epoch 52/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9918 - loss: 0.0254\n",
      "Epoch 52: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 1s/step - accuracy: 0.9918 - loss: 0.0254 - val_accuracy: 0.9979 - val_loss: 0.0081\n",
      "Epoch 53/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9975 - loss: 0.0099\n",
      "Epoch 53: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 1s/step - accuracy: 0.9975 - loss: 0.0099 - val_accuracy: 0.9984 - val_loss: 0.0059\n",
      "Epoch 54/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 0.0093\n",
      "Epoch 54: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 0.0093 - val_accuracy: 0.9930 - val_loss: 0.0291\n",
      "Epoch 55/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9939 - loss: 0.0277\n",
      "Epoch 55: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 1s/step - accuracy: 0.9939 - loss: 0.0277 - val_accuracy: 0.9975 - val_loss: 0.0078\n",
      "Epoch 56/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9960 - loss: 0.0132\n",
      "Epoch 56: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 1s/step - accuracy: 0.9960 - loss: 0.0132 - val_accuracy: 0.9971 - val_loss: 0.0075\n",
      "Epoch 57/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9977 - loss: 0.0076\n",
      "Epoch 57: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 1s/step - accuracy: 0.9976 - loss: 0.0076 - val_accuracy: 0.9971 - val_loss: 0.0149\n",
      "Epoch 58/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9963 - loss: 0.0121\n",
      "Epoch 58: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 1s/step - accuracy: 0.9963 - loss: 0.0121 - val_accuracy: 0.9979 - val_loss: 0.0081\n",
      "Epoch 59/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9979 - loss: 0.0062\n",
      "Epoch 59: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 1s/step - accuracy: 0.9979 - loss: 0.0063 - val_accuracy: 0.9988 - val_loss: 0.0075\n",
      "Epoch 60/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9960 - loss: 0.0126\n",
      "Epoch 60: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 1s/step - accuracy: 0.9960 - loss: 0.0127 - val_accuracy: 0.9979 - val_loss: 0.0102\n",
      "Epoch 61/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9907 - loss: 0.0277\n",
      "Epoch 61: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 1s/step - accuracy: 0.9907 - loss: 0.0276 - val_accuracy: 0.9955 - val_loss: 0.0143\n",
      "Epoch 62/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9949 - loss: 0.0155\n",
      "Epoch 62: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 1s/step - accuracy: 0.9949 - loss: 0.0155 - val_accuracy: 0.9967 - val_loss: 0.0125\n",
      "Epoch 63/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9967 - loss: 0.0105\n",
      "Epoch 63: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 1s/step - accuracy: 0.9967 - loss: 0.0105 - val_accuracy: 0.9975 - val_loss: 0.0085\n",
      "Epoch 64/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9991 - loss: 0.0051\n",
      "Epoch 64: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 1s/step - accuracy: 0.9991 - loss: 0.0051 - val_accuracy: 0.9988 - val_loss: 0.0077\n",
      "Epoch 65/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9965 - loss: 0.0101\n",
      "Epoch 65: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 1s/step - accuracy: 0.9965 - loss: 0.0101 - val_accuracy: 0.9979 - val_loss: 0.0050\n",
      "Epoch 66/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9932 - loss: 0.0199\n",
      "Epoch 66: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 1s/step - accuracy: 0.9931 - loss: 0.0199 - val_accuracy: 0.9951 - val_loss: 0.0192\n",
      "Epoch 67/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9932 - loss: 0.0219\n",
      "Epoch 67: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 1s/step - accuracy: 0.9932 - loss: 0.0219 - val_accuracy: 0.9979 - val_loss: 0.0121\n",
      "Epoch 68/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 0.0103\n",
      "Epoch 68: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 0.0103 - val_accuracy: 0.9979 - val_loss: 0.0069\n",
      "Epoch 69/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9957 - loss: 0.0119\n",
      "Epoch 69: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 1s/step - accuracy: 0.9957 - loss: 0.0119 - val_accuracy: 0.9975 - val_loss: 0.0102\n",
      "Epoch 70/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9953 - loss: 0.0135\n",
      "Epoch 70: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 1s/step - accuracy: 0.9953 - loss: 0.0135 - val_accuracy: 0.9979 - val_loss: 0.0102\n",
      "Epoch 71/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9935 - loss: 0.0157\n",
      "Epoch 71: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 1s/step - accuracy: 0.9935 - loss: 0.0157 - val_accuracy: 0.9979 - val_loss: 0.0073\n",
      "Epoch 72/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9981 - loss: 0.0071\n",
      "Epoch 72: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 1s/step - accuracy: 0.9981 - loss: 0.0071 - val_accuracy: 0.9971 - val_loss: 0.0162\n",
      "Epoch 73/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9983 - loss: 0.0049\n",
      "Epoch 73: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 1s/step - accuracy: 0.9983 - loss: 0.0049 - val_accuracy: 0.9971 - val_loss: 0.0075\n",
      "Epoch 74/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9960 - loss: 0.0151\n",
      "Epoch 74: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 1s/step - accuracy: 0.9960 - loss: 0.0151 - val_accuracy: 0.9967 - val_loss: 0.0129\n",
      "Epoch 75/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9967 - loss: 0.0114\n",
      "Epoch 75: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 1s/step - accuracy: 0.9967 - loss: 0.0114 - val_accuracy: 0.9975 - val_loss: 0.0131\n",
      "Epoch 76/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9952 - loss: 0.0165\n",
      "Epoch 76: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 1s/step - accuracy: 0.9952 - loss: 0.0165 - val_accuracy: 0.9967 - val_loss: 0.0136\n",
      "Epoch 77/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9925 - loss: 0.0207\n",
      "Epoch 77: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 1s/step - accuracy: 0.9926 - loss: 0.0207 - val_accuracy: 0.9959 - val_loss: 0.0153\n",
      "Epoch 78/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9964 - loss: 0.0122\n",
      "Epoch 78: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 1s/step - accuracy: 0.9964 - loss: 0.0122 - val_accuracy: 0.9967 - val_loss: 0.0140\n",
      "Epoch 79/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9950 - loss: 0.0112\n",
      "Epoch 79: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m472s\u001b[0m 2s/step - accuracy: 0.9950 - loss: 0.0112 - val_accuracy: 0.9975 - val_loss: 0.0111\n",
      "Epoch 80/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9978 - loss: 0.0067\n",
      "Epoch 80: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 1s/step - accuracy: 0.9978 - loss: 0.0067 - val_accuracy: 0.9975 - val_loss: 0.0120\n",
      "Epoch 81/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9973 - loss: 0.0087\n",
      "Epoch 81: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1295s\u001b[0m 4s/step - accuracy: 0.9973 - loss: 0.0087 - val_accuracy: 0.9979 - val_loss: 0.0122\n",
      "Epoch 82/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22s/step - accuracy: 0.9976 - loss: 0.0064 \n",
      "Epoch 82: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6749s\u001b[0m 22s/step - accuracy: 0.9976 - loss: 0.0064 - val_accuracy: 0.9979 - val_loss: 0.0078\n",
      "Epoch 83/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9971 - loss: 0.0110\n",
      "Epoch 83: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 1s/step - accuracy: 0.9971 - loss: 0.0110 - val_accuracy: 0.9984 - val_loss: 0.0057\n",
      "Epoch 84/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9982 - loss: 0.0064\n",
      "Epoch 84: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 1s/step - accuracy: 0.9982 - loss: 0.0064 - val_accuracy: 0.9984 - val_loss: 0.0082\n",
      "Epoch 85/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9962 - loss: 0.0140\n",
      "Epoch 85: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 1s/step - accuracy: 0.9962 - loss: 0.0140 - val_accuracy: 0.9963 - val_loss: 0.0115\n",
      "Epoch 86/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9949 - loss: 0.0153\n",
      "Epoch 86: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 1s/step - accuracy: 0.9949 - loss: 0.0153 - val_accuracy: 0.9984 - val_loss: 0.0071\n",
      "Epoch 87/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 0.0095\n",
      "Epoch 87: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 0.0094 - val_accuracy: 0.9979 - val_loss: 0.0091\n",
      "Epoch 88/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9966 - loss: 0.0105\n",
      "Epoch 88: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 1s/step - accuracy: 0.9966 - loss: 0.0105 - val_accuracy: 0.9984 - val_loss: 0.0090\n",
      "Epoch 89/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9942 - loss: 0.0234\n",
      "Epoch 89: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 1s/step - accuracy: 0.9942 - loss: 0.0234 - val_accuracy: 0.9984 - val_loss: 0.0092\n",
      "Epoch 90/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9959 - loss: 0.0181\n",
      "Epoch 90: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 1s/step - accuracy: 0.9959 - loss: 0.0181 - val_accuracy: 0.9988 - val_loss: 0.0043\n",
      "Epoch 91/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9966 - loss: 0.0150\n",
      "Epoch 91: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 1s/step - accuracy: 0.9966 - loss: 0.0150 - val_accuracy: 0.9984 - val_loss: 0.0066\n",
      "Epoch 92/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9978 - loss: 0.0067\n",
      "Epoch 92: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 1s/step - accuracy: 0.9978 - loss: 0.0067 - val_accuracy: 0.9963 - val_loss: 0.0127\n",
      "Epoch 93/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9987 - loss: 0.0036\n",
      "Epoch 93: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 1s/step - accuracy: 0.9987 - loss: 0.0036 - val_accuracy: 0.9959 - val_loss: 0.0210\n",
      "Epoch 94/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9973 - loss: 0.0084\n",
      "Epoch 94: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 1s/step - accuracy: 0.9973 - loss: 0.0084 - val_accuracy: 0.9971 - val_loss: 0.0088\n",
      "Epoch 95/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9979 - loss: 0.0063\n",
      "Epoch 95: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 1s/step - accuracy: 0.9979 - loss: 0.0063 - val_accuracy: 0.9975 - val_loss: 0.0094\n",
      "Epoch 96/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9942 - loss: 0.0275\n",
      "Epoch 96: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 1s/step - accuracy: 0.9942 - loss: 0.0275 - val_accuracy: 0.9963 - val_loss: 0.0109\n",
      "Epoch 97/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9939 - loss: 0.0174\n",
      "Epoch 97: val_accuracy did not improve from 0.99876\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 1s/step - accuracy: 0.9939 - loss: 0.0174 - val_accuracy: 0.9971 - val_loss: 0.0088\n",
      "Best model saved at: CNN2D_results/Scalogram_Spectrogram/best_model_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 01:01:10.325076: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-06-12 01:01:10.331120: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 374ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 01:01:43.652686: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n",
      "2025-06-12 01:01:43.653130: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 376ms/step - accuracy: 1.0000 - loss: 3.3807e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 01:03:38.781540: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n",
      "2025-06-12 01:03:38.781807: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 371ms/step - accuracy: 0.9991 - loss: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 01:04:07.849781: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n",
      "2025-06-12 01:04:07.850070: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 389ms/step - accuracy: 0.9985 - loss: 0.0040\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 01:04:48.459860: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n",
      "2025-06-12 01:04:48.460341: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6149 - loss: 1.0617"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 01:10:55.513005: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n",
      "2025-06-12 01:10:55.513679: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.96951, saving model to CNN2D_results/Scalogram_Spectrogram/best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 1s/step - accuracy: 0.6154 - loss: 1.0602 - val_accuracy: 0.9695 - val_loss: 0.1031\n",
      "Epoch 2/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9240 - loss: 0.2040\n",
      "Epoch 2: val_accuracy improved from 0.96951 to 0.99052, saving model to CNN2D_results/Scalogram_Spectrogram/best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 1s/step - accuracy: 0.9241 - loss: 0.2040 - val_accuracy: 0.9905 - val_loss: 0.0348\n",
      "Epoch 3/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9486 - loss: 0.1532\n",
      "Epoch 3: val_accuracy improved from 0.99052 to 0.99094, saving model to CNN2D_results/Scalogram_Spectrogram/best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 1s/step - accuracy: 0.9486 - loss: 0.1532 - val_accuracy: 0.9909 - val_loss: 0.0328\n",
      "Epoch 4/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9565 - loss: 0.1203\n",
      "Epoch 4: val_accuracy did not improve from 0.99094\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 1s/step - accuracy: 0.9565 - loss: 0.1203 - val_accuracy: 0.9876 - val_loss: 0.0318\n",
      "Epoch 5/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9653 - loss: 0.1052\n",
      "Epoch 5: val_accuracy improved from 0.99094 to 0.99258, saving model to CNN2D_results/Scalogram_Spectrogram/best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 1s/step - accuracy: 0.9653 - loss: 0.1052 - val_accuracy: 0.9926 - val_loss: 0.0178\n",
      "Epoch 6/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9685 - loss: 0.0906\n",
      "Epoch 6: val_accuracy did not improve from 0.99258\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 1s/step - accuracy: 0.9685 - loss: 0.0907 - val_accuracy: 0.9856 - val_loss: 0.0389\n",
      "Epoch 7/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9654 - loss: 0.0996\n",
      "Epoch 7: val_accuracy did not improve from 0.99258\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 1s/step - accuracy: 0.9654 - loss: 0.0996 - val_accuracy: 0.9522 - val_loss: 0.1216\n",
      "Epoch 8/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9649 - loss: 0.1067\n",
      "Epoch 8: val_accuracy did not improve from 0.99258\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 1s/step - accuracy: 0.9649 - loss: 0.1067 - val_accuracy: 0.9881 - val_loss: 0.0386\n",
      "Epoch 9/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9770 - loss: 0.0722\n",
      "Epoch 9: val_accuracy did not improve from 0.99258\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 1s/step - accuracy: 0.9770 - loss: 0.0722 - val_accuracy: 0.9876 - val_loss: 0.0449\n",
      "Epoch 10/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9721 - loss: 0.0834\n",
      "Epoch 10: val_accuracy did not improve from 0.99258\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 1s/step - accuracy: 0.9721 - loss: 0.0834 - val_accuracy: 0.9889 - val_loss: 0.0399\n",
      "Epoch 11/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9782 - loss: 0.0617\n",
      "Epoch 11: val_accuracy did not improve from 0.99258\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 1s/step - accuracy: 0.9782 - loss: 0.0617 - val_accuracy: 0.9860 - val_loss: 0.0445\n",
      "Epoch 12/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9742 - loss: 0.0752\n",
      "Epoch 12: val_accuracy improved from 0.99258 to 0.99629, saving model to CNN2D_results/Scalogram_Spectrogram/best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 1s/step - accuracy: 0.9742 - loss: 0.0752 - val_accuracy: 0.9963 - val_loss: 0.0136\n",
      "Epoch 13/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9844 - loss: 0.0482\n",
      "Epoch 13: val_accuracy did not improve from 0.99629\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 1s/step - accuracy: 0.9844 - loss: 0.0482 - val_accuracy: 0.9951 - val_loss: 0.0145\n",
      "Epoch 14/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9810 - loss: 0.0562\n",
      "Epoch 14: val_accuracy did not improve from 0.99629\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 1s/step - accuracy: 0.9810 - loss: 0.0562 - val_accuracy: 0.9839 - val_loss: 0.0484\n",
      "Epoch 15/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9855 - loss: 0.0467\n",
      "Epoch 15: val_accuracy did not improve from 0.99629\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 1s/step - accuracy: 0.9855 - loss: 0.0467 - val_accuracy: 0.9951 - val_loss: 0.0213\n",
      "Epoch 16/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9823 - loss: 0.0471\n",
      "Epoch 16: val_accuracy did not improve from 0.99629\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 1s/step - accuracy: 0.9823 - loss: 0.0471 - val_accuracy: 0.9938 - val_loss: 0.0260\n",
      "Epoch 17/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9851 - loss: 0.0405\n",
      "Epoch 17: val_accuracy did not improve from 0.99629\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 1s/step - accuracy: 0.9851 - loss: 0.0406 - val_accuracy: 0.9951 - val_loss: 0.0167\n",
      "Epoch 18/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9826 - loss: 0.0452\n",
      "Epoch 18: val_accuracy did not improve from 0.99629\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 1s/step - accuracy: 0.9826 - loss: 0.0453 - val_accuracy: 0.9934 - val_loss: 0.0221\n",
      "Epoch 19/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9844 - loss: 0.0446\n",
      "Epoch 19: val_accuracy did not improve from 0.99629\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 1s/step - accuracy: 0.9844 - loss: 0.0446 - val_accuracy: 0.9893 - val_loss: 0.0375\n",
      "Epoch 20/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9855 - loss: 0.0466\n",
      "Epoch 20: val_accuracy improved from 0.99629 to 0.99712, saving model to CNN2D_results/Scalogram_Spectrogram/best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 1s/step - accuracy: 0.9855 - loss: 0.0466 - val_accuracy: 0.9971 - val_loss: 0.0123\n",
      "Epoch 21/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9893 - loss: 0.0317\n",
      "Epoch 21: val_accuracy did not improve from 0.99712\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 1s/step - accuracy: 0.9893 - loss: 0.0317 - val_accuracy: 0.9831 - val_loss: 0.0597\n",
      "Epoch 22/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9894 - loss: 0.0304\n",
      "Epoch 22: val_accuracy did not improve from 0.99712\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 1s/step - accuracy: 0.9894 - loss: 0.0304 - val_accuracy: 0.9901 - val_loss: 0.0357\n",
      "Epoch 23/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37s/step - accuracy: 0.9909 - loss: 0.0279 \n",
      "Epoch 33: val_accuracy did not improve from 0.99712\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11191s\u001b[0m 37s/step - accuracy: 0.9909 - loss: 0.0279 - val_accuracy: 0.9959 - val_loss: 0.0183\n",
      "Epoch 34/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9888 - loss: 0.0331\n",
      "Epoch 34: val_accuracy did not improve from 0.99712\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 1s/step - accuracy: 0.9888 - loss: 0.0331 - val_accuracy: 0.9938 - val_loss: 0.0263\n",
      "Epoch 35/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9917 - loss: 0.0256\n",
      "Epoch 35: val_accuracy did not improve from 0.99712\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 1s/step - accuracy: 0.9917 - loss: 0.0256 - val_accuracy: 0.9930 - val_loss: 0.0314\n",
      "Epoch 36/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9921 - loss: 0.0215\n",
      "Epoch 36: val_accuracy did not improve from 0.99712\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 1s/step - accuracy: 0.9921 - loss: 0.0215 - val_accuracy: 0.9967 - val_loss: 0.0089\n",
      "Epoch 37/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9949 - loss: 0.0155\n",
      "Epoch 37: val_accuracy improved from 0.99712 to 0.99835, saving model to CNN2D_results/Scalogram_Spectrogram/best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 1s/step - accuracy: 0.9949 - loss: 0.0155 - val_accuracy: 0.9984 - val_loss: 0.0080\n",
      "Epoch 38/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9957 - loss: 0.0146\n",
      "Epoch 38: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 1s/step - accuracy: 0.9957 - loss: 0.0146 - val_accuracy: 0.9984 - val_loss: 0.0106\n",
      "Epoch 39/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9917 - loss: 0.0339\n",
      "Epoch 39: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 1s/step - accuracy: 0.9917 - loss: 0.0339 - val_accuracy: 0.9967 - val_loss: 0.0121\n",
      "Epoch 40/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9922 - loss: 0.0242\n",
      "Epoch 40: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 1s/step - accuracy: 0.9922 - loss: 0.0241 - val_accuracy: 0.9967 - val_loss: 0.0133\n",
      "Epoch 41/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9942 - loss: 0.0179\n",
      "Epoch 41: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 1s/step - accuracy: 0.9942 - loss: 0.0179 - val_accuracy: 0.9955 - val_loss: 0.0158\n",
      "Epoch 42/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9841 - loss: 0.0530\n",
      "Epoch 42: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 1s/step - accuracy: 0.9841 - loss: 0.0530 - val_accuracy: 0.9967 - val_loss: 0.0120\n",
      "Epoch 43/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9947 - loss: 0.0203\n",
      "Epoch 43: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 1s/step - accuracy: 0.9947 - loss: 0.0203 - val_accuracy: 0.9959 - val_loss: 0.0129\n",
      "Epoch 44/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9947 - loss: 0.0152\n",
      "Epoch 44: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m451s\u001b[0m 1s/step - accuracy: 0.9947 - loss: 0.0152 - val_accuracy: 0.9934 - val_loss: 0.0293\n",
      "Epoch 45/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9933 - loss: 0.0186\n",
      "Epoch 45: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 1s/step - accuracy: 0.9933 - loss: 0.0186 - val_accuracy: 0.9984 - val_loss: 0.0101\n",
      "Epoch 46/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9976 - loss: 0.0086\n",
      "Epoch 46: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 1s/step - accuracy: 0.9976 - loss: 0.0086 - val_accuracy: 0.9979 - val_loss: 0.0085\n",
      "Epoch 47/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9940 - loss: 0.0197\n",
      "Epoch 47: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m510s\u001b[0m 2s/step - accuracy: 0.9940 - loss: 0.0197 - val_accuracy: 0.9955 - val_loss: 0.0148\n",
      "Epoch 48/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9921 - loss: 0.0221\n",
      "Epoch 48: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m452s\u001b[0m 1s/step - accuracy: 0.9921 - loss: 0.0220 - val_accuracy: 0.9979 - val_loss: 0.0098\n",
      "Epoch 49/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9936 - loss: 0.0177\n",
      "Epoch 49: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 2s/step - accuracy: 0.9936 - loss: 0.0177 - val_accuracy: 0.9909 - val_loss: 0.0290\n",
      "Epoch 50/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9917 - loss: 0.0286\n",
      "Epoch 50: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 1s/step - accuracy: 0.9917 - loss: 0.0286 - val_accuracy: 0.9930 - val_loss: 0.0203\n",
      "Epoch 51/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9949 - loss: 0.0189\n",
      "Epoch 51: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m450s\u001b[0m 1s/step - accuracy: 0.9949 - loss: 0.0189 - val_accuracy: 0.9971 - val_loss: 0.0072\n",
      "Epoch 52/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9950 - loss: 0.0150\n",
      "Epoch 52: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m463s\u001b[0m 2s/step - accuracy: 0.9950 - loss: 0.0150 - val_accuracy: 0.9963 - val_loss: 0.0088\n",
      "Epoch 53/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9948 - loss: 0.0181\n",
      "Epoch 53: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 1s/step - accuracy: 0.9948 - loss: 0.0181 - val_accuracy: 0.9979 - val_loss: 0.0077\n",
      "Epoch 54/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9935 - loss: 0.0255\n",
      "Epoch 54: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 2s/step - accuracy: 0.9935 - loss: 0.0255 - val_accuracy: 0.9975 - val_loss: 0.0106\n",
      "Epoch 55/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9962 - loss: 0.0115\n",
      "Epoch 55: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 2s/step - accuracy: 0.9962 - loss: 0.0115 - val_accuracy: 0.9975 - val_loss: 0.0091\n",
      "Epoch 56/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9961 - loss: 0.0125\n",
      "Epoch 56: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 2s/step - accuracy: 0.9961 - loss: 0.0125 - val_accuracy: 0.9971 - val_loss: 0.0110\n",
      "Epoch 57/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9960 - loss: 0.0131\n",
      "Epoch 57: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m482s\u001b[0m 2s/step - accuracy: 0.9960 - loss: 0.0131 - val_accuracy: 0.9959 - val_loss: 0.0170\n",
      "Epoch 58/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9943 - loss: 0.0175\n",
      "Epoch 58: val_accuracy did not improve from 0.99835\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 2s/step - accuracy: 0.9943 - loss: 0.0175 - val_accuracy: 0.9946 - val_loss: 0.0217\n",
      "Epoch 59/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9920 - loss: 0.0238\n",
      "Epoch 59: val_accuracy improved from 0.99835 to 0.99918, saving model to CNN2D_results/Scalogram_Spectrogram/best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 1s/step - accuracy: 0.9920 - loss: 0.0238 - val_accuracy: 0.9992 - val_loss: 0.0067\n",
      "Epoch 60/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9946 - loss: 0.0146\n",
      "Epoch 60: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 1s/step - accuracy: 0.9946 - loss: 0.0146 - val_accuracy: 0.9930 - val_loss: 0.0319\n",
      "Epoch 61/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9925 - loss: 0.0198\n",
      "Epoch 61: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 1s/step - accuracy: 0.9925 - loss: 0.0198 - val_accuracy: 0.9959 - val_loss: 0.0186\n",
      "Epoch 62/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9958 - loss: 0.0183\n",
      "Epoch 62: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 1s/step - accuracy: 0.9958 - loss: 0.0183 - val_accuracy: 0.9955 - val_loss: 0.0155\n",
      "Epoch 63/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9960 - loss: 0.0121\n",
      "Epoch 63: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 1s/step - accuracy: 0.9960 - loss: 0.0121 - val_accuracy: 0.9979 - val_loss: 0.0087\n",
      "Epoch 64/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9960 - loss: 0.0121\n",
      "Epoch 64: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 1s/step - accuracy: 0.9960 - loss: 0.0121 - val_accuracy: 0.9901 - val_loss: 0.0429\n",
      "Epoch 65/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9927 - loss: 0.0263\n",
      "Epoch 65: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 1s/step - accuracy: 0.9927 - loss: 0.0262 - val_accuracy: 0.9975 - val_loss: 0.0068\n",
      "Epoch 66/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9982 - loss: 0.0065\n",
      "Epoch 66: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 1s/step - accuracy: 0.9982 - loss: 0.0065 - val_accuracy: 0.9971 - val_loss: 0.0095\n",
      "Epoch 67/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9954 - loss: 0.0117\n",
      "Epoch 67: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 2s/step - accuracy: 0.9954 - loss: 0.0117 - val_accuracy: 0.9984 - val_loss: 0.0135\n",
      "Epoch 68/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9962 - loss: 0.0131\n",
      "Epoch 68: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 1s/step - accuracy: 0.9962 - loss: 0.0131 - val_accuracy: 0.9979 - val_loss: 0.0083\n",
      "Epoch 69/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9981 - loss: 0.0070\n",
      "Epoch 69: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 1s/step - accuracy: 0.9981 - loss: 0.0070 - val_accuracy: 0.9909 - val_loss: 0.0304\n",
      "Epoch 70/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9938 - loss: 0.0217\n",
      "Epoch 70: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 1s/step - accuracy: 0.9938 - loss: 0.0217 - val_accuracy: 0.9967 - val_loss: 0.0103\n",
      "Epoch 71/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9957 - loss: 0.0164\n",
      "Epoch 71: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 2s/step - accuracy: 0.9957 - loss: 0.0164 - val_accuracy: 0.9988 - val_loss: 0.0074\n",
      "Epoch 72/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9976 - loss: 0.0127\n",
      "Epoch 72: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 2s/step - accuracy: 0.9976 - loss: 0.0127 - val_accuracy: 0.9988 - val_loss: 0.0066\n",
      "Epoch 73/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9969 - loss: 0.0094\n",
      "Epoch 73: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m508s\u001b[0m 2s/step - accuracy: 0.9969 - loss: 0.0094 - val_accuracy: 0.9942 - val_loss: 0.0255\n",
      "Epoch 74/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9949 - loss: 0.0215\n",
      "Epoch 74: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 1s/step - accuracy: 0.9949 - loss: 0.0215 - val_accuracy: 0.9992 - val_loss: 0.0033\n",
      "Epoch 75/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9971 - loss: 0.0107\n",
      "Epoch 75: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 1s/step - accuracy: 0.9971 - loss: 0.0107 - val_accuracy: 0.9971 - val_loss: 0.0115\n",
      "Epoch 76/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9968 - loss: 0.0113\n",
      "Epoch 76: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 1s/step - accuracy: 0.9968 - loss: 0.0113 - val_accuracy: 0.9955 - val_loss: 0.0189\n",
      "Epoch 77/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9961 - loss: 0.0132\n",
      "Epoch 77: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 1s/step - accuracy: 0.9961 - loss: 0.0132 - val_accuracy: 0.9691 - val_loss: 0.1857\n",
      "Epoch 78/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9878 - loss: 0.0472\n",
      "Epoch 78: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 1s/step - accuracy: 0.9878 - loss: 0.0471 - val_accuracy: 0.9979 - val_loss: 0.0074\n",
      "Epoch 79/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9949 - loss: 0.0150\n",
      "Epoch 79: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 1s/step - accuracy: 0.9949 - loss: 0.0150 - val_accuracy: 0.9975 - val_loss: 0.0080\n",
      "Epoch 80/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 0.0090\n",
      "Epoch 80: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 0.0090 - val_accuracy: 0.9959 - val_loss: 0.0166\n",
      "Epoch 81/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9975 - loss: 0.0076\n",
      "Epoch 81: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 2s/step - accuracy: 0.9975 - loss: 0.0076 - val_accuracy: 0.9967 - val_loss: 0.0102\n",
      "Epoch 82/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9983 - loss: 0.0088\n",
      "Epoch 82: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 2s/step - accuracy: 0.9983 - loss: 0.0088 - val_accuracy: 0.9988 - val_loss: 0.0075\n",
      "Epoch 83/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9981 - loss: 0.0077\n",
      "Epoch 83: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 2s/step - accuracy: 0.9981 - loss: 0.0077 - val_accuracy: 0.9979 - val_loss: 0.0077\n",
      "Epoch 84/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9968 - loss: 0.0107\n",
      "Epoch 84: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 1s/step - accuracy: 0.9968 - loss: 0.0108 - val_accuracy: 0.9963 - val_loss: 0.0126\n",
      "Epoch 85/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9938 - loss: 0.0232\n",
      "Epoch 85: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 1s/step - accuracy: 0.9938 - loss: 0.0232 - val_accuracy: 0.9967 - val_loss: 0.0101\n",
      "Epoch 86/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9953 - loss: 0.0152\n",
      "Epoch 86: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 2s/step - accuracy: 0.9953 - loss: 0.0152 - val_accuracy: 0.9967 - val_loss: 0.0109\n",
      "Epoch 87/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9962 - loss: 0.0128\n",
      "Epoch 87: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 2s/step - accuracy: 0.9962 - loss: 0.0128 - val_accuracy: 0.9967 - val_loss: 0.0133\n",
      "Epoch 88/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 0.0119\n",
      "Epoch 88: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 0.0119 - val_accuracy: 0.9946 - val_loss: 0.0160\n",
      "Epoch 89/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9923 - loss: 0.0290\n",
      "Epoch 89: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 1s/step - accuracy: 0.9923 - loss: 0.0290 - val_accuracy: 0.9905 - val_loss: 0.0290\n",
      "Epoch 90/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9959 - loss: 0.0129\n",
      "Epoch 90: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 1s/step - accuracy: 0.9959 - loss: 0.0129 - val_accuracy: 0.9971 - val_loss: 0.0109\n",
      "Epoch 91/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 0.0072\n",
      "Epoch 91: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 2s/step - accuracy: 0.9970 - loss: 0.0072 - val_accuracy: 0.9963 - val_loss: 0.0096\n",
      "Epoch 92/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9950 - loss: 0.0108\n",
      "Epoch 92: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 1s/step - accuracy: 0.9950 - loss: 0.0108 - val_accuracy: 0.9988 - val_loss: 0.0046\n",
      "Epoch 93/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9974 - loss: 0.0068\n",
      "Epoch 93: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 1s/step - accuracy: 0.9974 - loss: 0.0068 - val_accuracy: 0.9988 - val_loss: 0.0043\n",
      "Epoch 94/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9977 - loss: 0.0066\n",
      "Epoch 94: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 1s/step - accuracy: 0.9977 - loss: 0.0066 - val_accuracy: 0.9984 - val_loss: 0.0046\n",
      "Epoch 95/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9968 - loss: 0.0084\n",
      "Epoch 95: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 1s/step - accuracy: 0.9968 - loss: 0.0084 - val_accuracy: 0.9967 - val_loss: 0.0132\n",
      "Epoch 96/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9930 - loss: 0.0274\n",
      "Epoch 96: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m451s\u001b[0m 1s/step - accuracy: 0.9930 - loss: 0.0273 - val_accuracy: 0.9967 - val_loss: 0.0079\n",
      "Epoch 97/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9969 - loss: 0.0073\n",
      "Epoch 97: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 2s/step - accuracy: 0.9969 - loss: 0.0073 - val_accuracy: 0.9967 - val_loss: 0.0095\n",
      "Epoch 98/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9958 - loss: 0.0142\n",
      "Epoch 98: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 2s/step - accuracy: 0.9958 - loss: 0.0142 - val_accuracy: 0.9901 - val_loss: 0.0409\n",
      "Epoch 99/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9974 - loss: 0.0085\n",
      "Epoch 99: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m470s\u001b[0m 2s/step - accuracy: 0.9974 - loss: 0.0085 - val_accuracy: 0.9967 - val_loss: 0.0102\n",
      "Epoch 100/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9954 - loss: 0.0147\n",
      "Epoch 100: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m473s\u001b[0m 2s/step - accuracy: 0.9953 - loss: 0.0147 - val_accuracy: 0.9951 - val_loss: 0.0104\n",
      "Epoch 101/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9958 - loss: 0.0111\n",
      "Epoch 101: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m452s\u001b[0m 1s/step - accuracy: 0.9958 - loss: 0.0111 - val_accuracy: 0.9963 - val_loss: 0.0145\n",
      "Epoch 102/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9978 - loss: 0.0073\n",
      "Epoch 102: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m454s\u001b[0m 1s/step - accuracy: 0.9978 - loss: 0.0073 - val_accuracy: 0.9984 - val_loss: 0.0044\n",
      "Epoch 103/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9981 - loss: 0.0053\n",
      "Epoch 103: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 2s/step - accuracy: 0.9981 - loss: 0.0053 - val_accuracy: 0.9971 - val_loss: 0.0080\n",
      "Epoch 104/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9980 - loss: 0.0085\n",
      "Epoch 104: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 2s/step - accuracy: 0.9980 - loss: 0.0085 - val_accuracy: 0.9979 - val_loss: 0.0058\n",
      "Epoch 105/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9969 - loss: 0.0088\n",
      "Epoch 105: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m463s\u001b[0m 2s/step - accuracy: 0.9969 - loss: 0.0088 - val_accuracy: 0.9975 - val_loss: 0.0068\n",
      "Epoch 106/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9969 - loss: 0.0115\n",
      "Epoch 106: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 1s/step - accuracy: 0.9969 - loss: 0.0115 - val_accuracy: 0.9963 - val_loss: 0.0089\n",
      "Epoch 107/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9959 - loss: 0.0131\n",
      "Epoch 107: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 1s/step - accuracy: 0.9959 - loss: 0.0131 - val_accuracy: 0.9975 - val_loss: 0.0085\n",
      "Epoch 108/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9937 - loss: 0.0320\n",
      "Epoch 108: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 1s/step - accuracy: 0.9937 - loss: 0.0320 - val_accuracy: 0.9971 - val_loss: 0.0102\n",
      "Epoch 109/200\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9947 - loss: 0.0145\n",
      "Epoch 109: val_accuracy did not improve from 0.99918\n",
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 1s/step - accuracy: 0.9947 - loss: 0.0145 - val_accuracy: 0.9984 - val_loss: 0.0060\n",
      "Best model saved at: CNN2D_results/Scalogram_Spectrogram/best_model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 17:09:19.535730: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-06-12 17:09:19.537621: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 2048, but received input with shape (32, 18432)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 96, 96, 1), dtype=float32)\n  • training=False\n  • mask=None\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest model loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m fl2 = fl1 + \u001b[38;5;28mlen\u001b[39m(test)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m pred_all_val[fl1:fl2, :] = \u001b[43mCNN_2D_best_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_2D_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m y_2D_val[fl1:fl2, :] = y_2D_train[test]\n\u001b[32m     62\u001b[39m kfold_test_len.append(fl2 - fl1)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyenvs/tf-env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyenvs/tf-env/lib/python3.11/site-packages/keras/src/layers/input_spec.py:227\u001b[39m, in \u001b[36massert_input_compatibility\u001b[39m\u001b[34m(input_spec, inputs, layer_name)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec.axes.items():\n\u001b[32m    223\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[32m    224\u001b[39m             value,\n\u001b[32m    225\u001b[39m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    226\u001b[39m         }:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    228\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of layer \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m is \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    229\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    230\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    231\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbut received input with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    232\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    233\u001b[39m             )\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spec.shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 2048, but received input with shape (32, 18432)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 96, 96, 1), dtype=float32)\n  • training=False\n  • mask=None\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "interval_length = 320\n",
    "samples_per_block = 1600\n",
    "image_type = 'scalogram'  # Change to 'spectrogram' to use spectrograms\n",
    "image_shape = (96, 96)\n",
    "kSplits = 5\n",
    "foldername = \"CNN2D_results/Scalogram_Spectrogram/\"\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint #Saves the model with the highest validation accuracy for each fold\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "from tensorflow.keras.models import load_model \n",
    "\n",
    "# Load and prepare data\n",
    "Data = ImportData()\n",
    "X, Y_CNN, Y = DataPreparation(Data, interval_length, samples_per_block, image_type=image_type)\n",
    "Input_2D = X.reshape([-1, image_shape[0], image_shape[1], 1])\n",
    "print(f\"Shape of Input Data: {Input_2D.shape}\")\n",
    "\n",
    "# Train-test split\n",
    "X_2D_train, X_2D_test, y_2D_train, y_2D_test, y_label_train, y_label_test = train_test_split(\n",
    "    Input_2D, Y_CNN, Y, train_size=0.8, test_size=0.2, random_state=42, stratify=Y\n",
    ")\n",
    "\n",
    "# K-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=kSplits, random_state=42, shuffle=True)\n",
    "accuracy_train = []\n",
    "accuracy_val = []\n",
    "accuracy_test = []\n",
    "pred_all_val = np.zeros([len(X_2D_train), 10])\n",
    "y_2D_val = np.zeros([len(X_2D_train), 10])\n",
    "kfold_test_len = []\n",
    "fl1 = 0\n",
    "k = 1\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=50, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "for fold, (train, test) in enumerate(kfold.split(X_2D_train, y_label_train)):\n",
    "    checkpoint_filepath = foldername + f\"best_model_{k}.h5\"\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "    Classification_2D = CNN_2D()\n",
    "    history = Classification_2D.model.fit(\n",
    "        X_2D_train[train], y_2D_train[train],\n",
    "        validation_data=(X_2D_train[test], y_2D_train[test]),\n",
    "        epochs=200,\n",
    "        verbose=1,\n",
    "        callbacks=[checkpoint, early_stop]\n",
    "    )\n",
    "    print(f\"Best model saved at: {checkpoint_filepath}\")\n",
    "    CNN_2D_best_model = load_model(checkpoint_filepath)\n",
    "    print(\"Best model loaded successfully!\")\n",
    "    \n",
    "    fl2 = fl1 + len(test)\n",
    "    pred_all_val[fl1:fl2, :] = CNN_2D_best_model.predict(X_2D_train[test])\n",
    "    y_2D_val[fl1:fl2, :] = y_2D_train[test]\n",
    "    kfold_test_len.append(fl2 - fl1)\n",
    "    fl1 = fl2\n",
    "\n",
    "    train_loss, train_accuracy = CNN_2D_best_model.evaluate(X_2D_train[train], y_2D_train[train])\n",
    "    accuracy_train.append(train_accuracy)\n",
    "    val_loss, val_accuracy = CNN_2D_best_model.evaluate(X_2D_train[test], y_2D_train[test])\n",
    "    accuracy_val.append(val_accuracy)\n",
    "    test_loss, test_accuracy = CNN_2D_best_model.evaluate(X_2D_test, y_2D_test)\n",
    "    accuracy_test.append(test_accuracy)\n",
    "    \n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Evaluate the trained model by computing average accuracies across folds for training, validation, and test sets. Visualize performance with confusion matrices for train and test sets, and bar plots for fold-wise and train vs. test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN 2D train accuracy = 100.0\n",
      "CNN 2D validation accuracy = 99.8763918876648\n",
      "CNN 2D test accuracy = 99.86816048622131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 19:33:40.458149: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n",
      "2025-06-12 19:33:40.461007: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_14}}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 2048, but received input with shape (32, 18432)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 96, 96, 1), dtype=float32)\n  • training=False\n  • mask=None\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m plt.figure(\u001b[32m5\u001b[39m)\n\u001b[32m     15\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33mConfusion Matrix - CNN 2D Train\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m sns.heatmap(\u001b[43mConfusionMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCNN_2D_best_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_2D_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_2D_train\u001b[49m\u001b[43m)\u001b[49m, annot=\u001b[38;5;28;01mTrue\u001b[39;00m, fmt=\u001b[33m'\u001b[39m\u001b[33md\u001b[39m\u001b[33m'\u001b[39m, annot_kws={\u001b[33m\"\u001b[39m\u001b[33mfontsize\u001b[39m\u001b[33m\"\u001b[39m:\u001b[32m8\u001b[39m}, cmap=\u001b[33m\"\u001b[39m\u001b[33mYlGnBu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m plt.show()\n\u001b[32m     19\u001b[39m plt.figure(\u001b[32m6\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mConfusionMatrix\u001b[39m\u001b[34m(Model, X, y)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mConfusionMatrix\u001b[39m(Model, X, y):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     y_pred = np.argmax(\u001b[43mModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m     10\u001b[39m     ConfusionMat = confusion_matrix(np.argmax(y, axis=\u001b[32m1\u001b[39m), y_pred)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ConfusionMat\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyenvs/tf-env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyenvs/tf-env/lib/python3.11/site-packages/keras/src/layers/input_spec.py:227\u001b[39m, in \u001b[36massert_input_compatibility\u001b[39m\u001b[34m(input_spec, inputs, layer_name)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec.axes.items():\n\u001b[32m    223\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[32m    224\u001b[39m             value,\n\u001b[32m    225\u001b[39m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    226\u001b[39m         }:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    228\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of layer \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m is \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    229\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    230\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    231\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbut received input with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    232\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    233\u001b[39m             )\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spec.shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 2048, but received input with shape (32, 18432)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 96, 96, 1), dtype=float32)\n  • training=False\n  • mask=None\n  • kwargs=<class 'inspect._empty'>"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CNN_2D_train_accuracy = np.average(accuracy_train) * 100\n",
    "print(f'CNN 2D train accuracy = {CNN_2D_train_accuracy}')\n",
    "CNN_2D_val_accuracy = np.average(accuracy_val) * 100\n",
    "print(f'CNN 2D validation accuracy = {CNN_2D_val_accuracy}')\n",
    "CNN_2D_test_accuracy = np.average(accuracy_test) * 100\n",
    "print(f'CNN 2D test accuracy = {CNN_2D_test_accuracy}')\n",
    "\n",
    "def ConfusionMatrix(Model, X, y):\n",
    "    y_pred = np.argmax(Model.predict(X), axis=1)\n",
    "    ConfusionMat = confusion_matrix(np.argmax(y, axis=1), y_pred)\n",
    "    return ConfusionMat\n",
    "\n",
    "# Plot results\n",
    "plt.figure(5)\n",
    "plt.title('Confusion Matrix - CNN 2D Train')\n",
    "sns.heatmap(ConfusionMatrix(CNN_2D_best_model, X_2D_train, y_2D_train), annot=True, fmt='d', annot_kws={\"fontsize\":8}, cmap=\"YlGnBu\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(6)\n",
    "plt.title('Confusion Matrix - CNN 2D Test')\n",
    "sns.heatmap(ConfusionMatrix(CNN_2D_best_model, X_2D_test, y_2D_test), annot=True, fmt='d', annot_kws={\"fontsize\":8}, cmap=\"YlGnBu\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(7)\n",
    "plt.title('Train - Accuracy - CNN 2D')\n",
    "plt.bar(np.arange(1, kSplits + 1), [i * 100 for i in accuracy_val])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('folds')\n",
    "plt.ylim([70, 100])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(8)\n",
    "plt.title('Train vs Test Accuracy - CNN 2D')\n",
    "plt.bar([1, 2], [CNN_2D_train_accuracy, CNN_2D_test_accuracy])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('folds')\n",
    "plt.xticks([1, 2], ['Train', 'Test'])\n",
    "plt.ylim([70, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **Running the Notebook**: Ensure the CWRU dataset files are in the 'CWRU_BearingData_Load_2HP' folder. Install required libraries (e.g., `pip install PyWavelets` for pywt).\n",
    "- **Switching Input Types**: Change `image_type` to 'spectrogram' in the main execution cell to use spectrograms instead of scalograms.\n",
    "- **Performance**: Per Verstraete et al., scalograms achieve up to 99.5% accuracy on CWRU data with 96x96 images, outperforming spectrograms (99.5%) and HHT (97.6%).\n",
    "- **Scalability**: 96x96 images increase computational load. For lower memory usage, set `image_shape = (32, 32)`, but expect reduced accuracy (e.g., 98.8% for scalograms).\n",
    "\n",
    "This notebook provides an interactive environment to experiment with time-frequency-based fault diagnosis, leveraging the strengths of CNNs for automatic feature learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
